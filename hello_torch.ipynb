{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5503b3ec-6144-4b22-acb8-fd87bf1df9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183bd0b-4a24-4474-86ea-ecb4fbaeb595",
   "metadata": {},
   "source": [
    "## Импорт Датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c902e8de-83af-490f-849e-58185291373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\", \n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor(),\n",
    ")\n",
    "\n",
    "testing_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa03245c-7179-4287-a844-a5df3450d05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: torch.Size([64, 1, 28, 28]) | y_test shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size = batch_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(testing_data, batch_size = batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"X_test shape: {X.shape} | y_test shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96a24b-559e-4ce2-ba21-08047f2002bd",
   "metadata": {},
   "source": [
    "## Классическая нейронная сеть (Полносвязные слои с функцией активации ReLU)\n",
    "\n",
    "Для обучения используется SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b05ce973-3e1e-4062-bca0-d19d72b26b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28*28, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        logits = self.linear_relu_stack(X)\n",
    "\n",
    "        return logits\n",
    "\n",
    "nn_model = NeuralNet().to(device)\n",
    "\n",
    "print(nn_model)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09a83de0-f075-493f-b2f6-6aafde5c50a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      " ===============================================\n",
      "Loss: 2.310110569000244 | 64 / 60000\n",
      "Loss: 2.293985366821289 | 6464 / 60000\n",
      "Loss: 2.275315046310425 | 12864 / 60000\n",
      "Loss: 2.262678623199463 | 19264 / 60000\n",
      "Loss: 2.242948293685913 | 25664 / 60000\n",
      "Loss: 2.227524995803833 | 32064 / 60000\n",
      "Loss: 2.2212908267974854 | 38464 / 60000\n",
      "Loss: 2.195888042449951 | 44864 / 60000\n",
      "Loss: 2.184767246246338 | 51264 / 60000\n",
      "Loss: 2.1555087566375732 | 57664 / 60000\n",
      "Test || Loss: 2.152703 || Accuracy: 58.46%\n",
      "Epoch 2 \n",
      " ===============================================\n",
      "Loss: 2.1625683307647705 | 64 / 60000\n",
      "Loss: 2.146975517272949 | 6464 / 60000\n",
      "Loss: 2.0913848876953125 | 12864 / 60000\n",
      "Loss: 2.1056289672851562 | 19264 / 60000\n",
      "Loss: 2.052797794342041 | 25664 / 60000\n",
      "Loss: 2.0085554122924805 | 32064 / 60000\n",
      "Loss: 2.0213403701782227 | 38464 / 60000\n",
      "Loss: 1.9461053609848022 | 44864 / 60000\n",
      "Loss: 1.9373857975006104 | 51264 / 60000\n",
      "Loss: 1.8832042217254639 | 57664 / 60000\n",
      "Test || Loss: 1.874549 || Accuracy: 60.83%\n",
      "Epoch 3 \n",
      " ===============================================\n",
      "Loss: 1.9010343551635742 | 64 / 60000\n",
      "Loss: 1.8666703701019287 | 6464 / 60000\n",
      "Loss: 1.7512346506118774 | 12864 / 60000\n",
      "Loss: 1.7984598875045776 | 19264 / 60000\n",
      "Loss: 1.6950398683547974 | 25664 / 60000\n",
      "Loss: 1.6496260166168213 | 32064 / 60000\n",
      "Loss: 1.6652164459228516 | 38464 / 60000\n",
      "Loss: 1.5677945613861084 | 44864 / 60000\n",
      "Loss: 1.583914041519165 | 51264 / 60000\n",
      "Loss: 1.4956613779067993 | 57664 / 60000\n",
      "Test || Loss: 1.508577 || Accuracy: 62.20%\n",
      "Epoch 4 \n",
      " ===============================================\n",
      "Loss: 1.5670355558395386 | 64 / 60000\n",
      "Loss: 1.5347315073013306 | 6464 / 60000\n",
      "Loss: 1.3873364925384521 | 12864 / 60000\n",
      "Loss: 1.4691171646118164 | 19264 / 60000\n",
      "Loss: 1.3566707372665405 | 25664 / 60000\n",
      "Loss: 1.344698190689087 | 32064 / 60000\n",
      "Loss: 1.359865665435791 | 38464 / 60000\n",
      "Loss: 1.285265326499939 | 44864 / 60000\n",
      "Loss: 1.314612627029419 | 51264 / 60000\n",
      "Loss: 1.2226262092590332 | 57664 / 60000\n",
      "Test || Loss: 1.247414 || Accuracy: 63.42%\n",
      "Epoch 5 \n",
      " ===============================================\n",
      "Loss: 1.3168895244598389 | 64 / 60000\n",
      "Loss: 1.3027172088623047 | 6464 / 60000\n",
      "Loss: 1.1385904550552368 | 12864 / 60000\n",
      "Loss: 1.2529560327529907 | 19264 / 60000\n",
      "Loss: 1.1327509880065918 | 25664 / 60000\n",
      "Loss: 1.1466737985610962 | 32064 / 60000\n",
      "Loss: 1.1705310344696045 | 38464 / 60000\n",
      "Loss: 1.1087300777435303 | 44864 / 60000\n",
      "Loss: 1.145089864730835 | 51264 / 60000\n",
      "Loss: 1.0608357191085815 | 57664 / 60000\n",
      "Test || Loss: 1.083709 || Accuracy: 64.64%\n",
      "Epoch 6 \n",
      " ===============================================\n",
      "Loss: 1.1490564346313477 | 64 / 60000\n",
      "Loss: 1.1542574167251587 | 6464 / 60000\n",
      "Loss: 0.9727306962013245 | 12864 / 60000\n",
      "Loss: 1.1157639026641846 | 19264 / 60000\n",
      "Loss: 0.9937199354171753 | 25664 / 60000\n",
      "Loss: 1.014049768447876 | 32064 / 60000\n",
      "Loss: 1.0534214973449707 | 38464 / 60000\n",
      "Loss: 0.9957171678543091 | 44864 / 60000\n",
      "Loss: 1.0345112085342407 | 51264 / 60000\n",
      "Loss: 0.9597434997558594 | 57664 / 60000\n",
      "Test || Loss: 0.978240 || Accuracy: 65.81%\n",
      "Epoch 7 \n",
      " ===============================================\n",
      "Loss: 1.0322469472885132 | 64 / 60000\n",
      "Loss: 1.0572419166564941 | 6464 / 60000\n",
      "Loss: 0.858657956123352 | 12864 / 60000\n",
      "Loss: 1.0249769687652588 | 19264 / 60000\n",
      "Loss: 0.9062327742576599 | 25664 / 60000\n",
      "Loss: 0.9210774898529053 | 32064 / 60000\n",
      "Loss: 0.9772071838378906 | 38464 / 60000\n",
      "Loss: 0.9220439791679382 | 44864 / 60000\n",
      "Loss: 0.9583472013473511 | 51264 / 60000\n",
      "Loss: 0.8923904895782471 | 57664 / 60000\n",
      "Test || Loss: 0.906666 || Accuracy: 67.20%\n",
      "Epoch 8 \n",
      " ===============================================\n",
      "Loss: 0.9461959600448608 | 64 / 60000\n",
      "Loss: 0.9896365404129028 | 6464 / 60000\n",
      "Loss: 0.7768231630325317 | 12864 / 60000\n",
      "Loss: 0.9612730741500854 | 19264 / 60000\n",
      "Loss: 0.8480961322784424 | 25664 / 60000\n",
      "Loss: 0.8530434370040894 | 32064 / 60000\n",
      "Loss: 0.9236173033714294 | 38464 / 60000\n",
      "Loss: 0.8724569082260132 | 44864 / 60000\n",
      "Loss: 0.902986466884613 | 51264 / 60000\n",
      "Loss: 0.8438960909843445 | 57664 / 60000\n",
      "Test || Loss: 0.855171 || Accuracy: 68.43%\n"
     ]
    }
   ],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(nn_model.parameters(), lr = 1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_func, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        preds = model(X)\n",
    "        loss = loss_func(preds, y)\n",
    "\n",
    "        #backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"Loss: {loss} | {current} / {size}\")\n",
    "\n",
    "def test(dataloader, model, loss_func):\n",
    "    size = len(dataloader.dataset)\n",
    "    batches_amount = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            preds = model(X)\n",
    "            test_loss += loss_func(preds, y).item()\n",
    "            correct += (preds.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= batches_amount\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test || Loss: {test_loss:>5f} || Accuracy: {correct*100:>.2f}%\")\n",
    "\n",
    "\n",
    "epochs = 8\n",
    "for e in range(epochs):\n",
    "    print(f\"Epoch {e+1} \\n ===============================================\")\n",
    "    train(train_dataloader, nn_model, loss_func, optimizer)\n",
    "    test(test_dataloader, nn_model, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d48baa-6d3a-4f4a-ae19-2deef0f89547",
   "metadata": {},
   "source": [
    "## Свёрточная нейронная сеть (+ использование нормирования)\n",
    "\n",
    "Для обучения использовалась Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "071789cf-3a44-448f-9931-bdb9e23443a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [ToTensor(),\n",
    "    torchvision.transforms.Normalize((.5), (.5))]\n",
    ")\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = \"data_cnn\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root = \"data_cnn\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f07934d3-c5f7-465b-b6d2-3f669f3e5ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: torch.Size([64, 1, 28, 28]) | y_test.shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size = batch_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size = batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"X_test.shape: {X.shape} | y_test.shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1a08442-846f-4346-82c1-60b8c9d726cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNet, self).__init__()\n",
    "        \n",
    "        self.first_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        )\n",
    "\n",
    "        self.second_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 0),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size = 2),\n",
    "        )\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(in_features = 64*6*6, out_features = 600)\n",
    "        self.drop = torch.nn.Dropout2d(.25)\n",
    "        self.fc2 = torch.nn.Linear(in_features = 600, out_features = 120)\n",
    "        self.fc3 = torch.nn.Linear(in_features = 120, out_features = 10)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        out_sequential1 = self.first_layer(X)\n",
    "        out_sequential2 = self.second_layer(out_sequential1)\n",
    "        out_sequential2 = out_sequential2.view(out_sequential2.size(0), -1)\n",
    "        out_fc1 = self.fc1(out_sequential2)\n",
    "        out_drop = self.drop(out_fc1)\n",
    "        out_fc2 = self.fc2(out_drop)\n",
    "        out_fc3 = self.fc3(out_fc2)\n",
    "\n",
    "        return out_fc3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3bebd5-0e04-402e-9f4b-0f046998bd40",
   "metadata": {},
   "source": [
    "## Переопределим некоторые уже известные переменные для удобства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a5ed954-6ed7-42ff-b62a-4bbeb4bd8934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalNet(\n",
      "  (first_layer): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (second_layer): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=2304, out_features=600, bias=True)\n",
      "  (drop): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=600, out_features=120, bias=True)\n",
      "  (fc3): Linear(in_features=120, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_model = ConvolutionalNet().to(device)\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb0a7442-f9a6-4cf5-9808-2df96f70a609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      " ===================================\n",
      "Loss: 2.295938491821289 | 64 / 60000\n",
      "Loss: 0.5700045824050903 | 6464 / 60000\n",
      "Loss: 0.2895480692386627 | 12864 / 60000\n",
      "Loss: 0.5124019384384155 | 19264 / 60000\n",
      "Loss: 0.4573103189468384 | 25664 / 60000\n",
      "Loss: 0.47280430793762207 | 32064 / 60000\n",
      "Loss: 0.24157072603702545 | 38464 / 60000\n",
      "Loss: 0.6042357087135315 | 44864 / 60000\n",
      "Loss: 0.3585544228553772 | 51264 / 60000\n",
      "Loss: 0.2593194544315338 | 57664 / 60000\n",
      "Test || Loss: 0.387314 || Accuracy: 85.69%\n",
      "Epoch 2 \n",
      " ===================================\n",
      "Loss: 0.2269202470779419 | 64 / 60000\n",
      "Loss: 0.3157084882259369 | 6464 / 60000\n",
      "Loss: 0.21053919196128845 | 12864 / 60000\n",
      "Loss: 0.38154926896095276 | 19264 / 60000\n",
      "Loss: 0.49371281266212463 | 25664 / 60000\n",
      "Loss: 0.4264756143093109 | 32064 / 60000\n",
      "Loss: 0.21364636719226837 | 38464 / 60000\n",
      "Loss: 0.46278896927833557 | 44864 / 60000\n",
      "Loss: 0.2566431164741516 | 51264 / 60000\n",
      "Loss: 0.2287500500679016 | 57664 / 60000\n",
      "Test || Loss: 0.320918 || Accuracy: 88.28%\n",
      "Epoch 3 \n",
      " ===================================\n",
      "Loss: 0.18783168494701385 | 64 / 60000\n",
      "Loss: 0.3981507420539856 | 6464 / 60000\n",
      "Loss: 0.1608162522315979 | 12864 / 60000\n",
      "Loss: 0.2856365144252777 | 19264 / 60000\n",
      "Loss: 0.4742153286933899 | 25664 / 60000\n",
      "Loss: 0.39576438069343567 | 32064 / 60000\n",
      "Loss: 0.2103082686662674 | 38464 / 60000\n",
      "Loss: 0.3334009051322937 | 44864 / 60000\n",
      "Loss: 0.1438135802745819 | 51264 / 60000\n",
      "Loss: 0.23402932286262512 | 57664 / 60000\n",
      "Test || Loss: 0.319947 || Accuracy: 89.13%\n",
      "Epoch 4 \n",
      " ===================================\n",
      "Loss: 0.16952009499073029 | 64 / 60000\n",
      "Loss: 0.26417118310928345 | 6464 / 60000\n",
      "Loss: 0.15931817889213562 | 12864 / 60000\n",
      "Loss: 0.19085752964019775 | 19264 / 60000\n",
      "Loss: 0.446713387966156 | 25664 / 60000\n",
      "Loss: 0.37657538056373596 | 32064 / 60000\n",
      "Loss: 0.18878789246082306 | 38464 / 60000\n",
      "Loss: 0.37926554679870605 | 44864 / 60000\n",
      "Loss: 0.16990728676319122 | 51264 / 60000\n",
      "Loss: 0.17898158729076385 | 57664 / 60000\n",
      "Test || Loss: 0.307978 || Accuracy: 89.60%\n",
      "Epoch 5 \n",
      " ===================================\n",
      "Loss: 0.20942242443561554 | 64 / 60000\n",
      "Loss: 0.25096869468688965 | 6464 / 60000\n",
      "Loss: 0.14575760066509247 | 12864 / 60000\n",
      "Loss: 0.1746533066034317 | 19264 / 60000\n",
      "Loss: 0.29954299330711365 | 25664 / 60000\n",
      "Loss: 0.3565237522125244 | 32064 / 60000\n",
      "Loss: 0.1704108566045761 | 38464 / 60000\n",
      "Loss: 0.32067790627479553 | 44864 / 60000\n",
      "Loss: 0.24631069600582123 | 51264 / 60000\n",
      "Loss: 0.1936231255531311 | 57664 / 60000\n",
      "Test || Loss: 0.311826 || Accuracy: 89.93%\n"
     ]
    }
   ],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr = 1e-3)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    print(f\"Epoch {e+1} \\n ===================================\")\n",
    "    train(train_dataloader, cnn_model, loss_func, optimizer)\n",
    "    test(test_dataloader, cnn_model, loss_func)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
