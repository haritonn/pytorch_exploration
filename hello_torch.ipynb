{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5503b3ec-6144-4b22-acb8-fd87bf1df9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183bd0b-4a24-4474-86ea-ecb4fbaeb595",
   "metadata": {},
   "source": [
    "## Импорт Датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c902e8de-83af-490f-849e-58185291373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\", \n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor(),\n",
    ")\n",
    "\n",
    "testing_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa03245c-7179-4287-a844-a5df3450d05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: torch.Size([64, 1, 28, 28]) | y_test shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size = batch_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(testing_data, batch_size = batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"X_test shape: {X.shape} | y_test shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96a24b-559e-4ce2-ba21-08047f2002bd",
   "metadata": {},
   "source": [
    "## Классическая нейронная сеть (Полносвязные слои с функцией активации ReLU)\n",
    "\n",
    "Для обучения используется SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b05ce973-3e1e-4062-bca0-d19d72b26b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28*28, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        logits = self.linear_relu_stack(X)\n",
    "\n",
    "        return logits\n",
    "\n",
    "nn_model = NeuralNet().to(device)\n",
    "\n",
    "print(nn_model)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09a83de0-f075-493f-b2f6-6aafde5c50a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      " ===============================================\n",
      "Loss: 2.304490804672241 | 64 / 60000\n",
      "Loss: 2.292654037475586 | 6464 / 60000\n",
      "Loss: 2.2741780281066895 | 12864 / 60000\n",
      "Loss: 2.2638673782348633 | 19264 / 60000\n",
      "Loss: 2.2513375282287598 | 25664 / 60000\n",
      "Loss: 2.212019443511963 | 32064 / 60000\n",
      "Loss: 2.2316904067993164 | 38464 / 60000\n",
      "Loss: 2.189394235610962 | 44864 / 60000\n",
      "Loss: 2.1826353073120117 | 51264 / 60000\n",
      "Loss: 2.148253917694092 | 57664 / 60000\n",
      "Test || Loss: 2.148624 || Accuracy: 30.02%\n",
      "Epoch 2 \n",
      " ===============================================\n",
      "Loss: 2.1606557369232178 | 64 / 60000\n",
      "Loss: 2.153538227081299 | 6464 / 60000\n",
      "Loss: 2.098604440689087 | 12864 / 60000\n",
      "Loss: 2.105241060256958 | 19264 / 60000\n",
      "Loss: 2.0628411769866943 | 25664 / 60000\n",
      "Loss: 1.9925140142440796 | 32064 / 60000\n",
      "Loss: 2.0258476734161377 | 38464 / 60000\n",
      "Loss: 1.9448362588882446 | 44864 / 60000\n",
      "Loss: 1.9432240724563599 | 51264 / 60000\n",
      "Loss: 1.8591110706329346 | 57664 / 60000\n",
      "Test || Loss: 1.876250 || Accuracy: 55.48%\n",
      "Epoch 3 \n",
      " ===============================================\n",
      "Loss: 1.9106634855270386 | 64 / 60000\n",
      "Loss: 1.8799083232879639 | 6464 / 60000\n",
      "Loss: 1.7715507745742798 | 12864 / 60000\n",
      "Loss: 1.7989544868469238 | 19264 / 60000\n",
      "Loss: 1.6941930055618286 | 25664 / 60000\n",
      "Loss: 1.6478840112686157 | 32064 / 60000\n",
      "Loss: 1.6673471927642822 | 38464 / 60000\n",
      "Loss: 1.5764738321304321 | 44864 / 60000\n",
      "Loss: 1.5902270078659058 | 51264 / 60000\n",
      "Loss: 1.4791239500045776 | 57664 / 60000\n",
      "Test || Loss: 1.513218 || Accuracy: 61.76%\n",
      "Epoch 4 \n",
      " ===============================================\n",
      "Loss: 1.5788202285766602 | 64 / 60000\n",
      "Loss: 1.5421744585037231 | 6464 / 60000\n",
      "Loss: 1.4027438163757324 | 12864 / 60000\n",
      "Loss: 1.4616090059280396 | 19264 / 60000\n",
      "Loss: 1.3482364416122437 | 25664 / 60000\n",
      "Loss: 1.3520517349243164 | 32064 / 60000\n",
      "Loss: 1.360739827156067 | 38464 / 60000\n",
      "Loss: 1.2926522493362427 | 44864 / 60000\n",
      "Loss: 1.3151057958602905 | 51264 / 60000\n",
      "Loss: 1.21903395652771 | 57664 / 60000\n",
      "Test || Loss: 1.250959 || Accuracy: 63.01%\n",
      "Epoch 5 \n",
      " ===============================================\n",
      "Loss: 1.3263146877288818 | 64 / 60000\n",
      "Loss: 1.3074016571044922 | 6464 / 60000\n",
      "Loss: 1.1467512845993042 | 12864 / 60000\n",
      "Loss: 1.2428832054138184 | 19264 / 60000\n",
      "Loss: 1.1248701810836792 | 25664 / 60000\n",
      "Loss: 1.155645489692688 | 32064 / 60000\n",
      "Loss: 1.1702736616134644 | 38464 / 60000\n",
      "Loss: 1.1132361888885498 | 44864 / 60000\n",
      "Loss: 1.1423345804214478 | 51264 / 60000\n",
      "Loss: 1.064025640487671 | 57664 / 60000\n",
      "Test || Loss: 1.087551 || Accuracy: 64.28%\n",
      "Epoch 6 \n",
      " ===============================================\n",
      "Loss: 1.1563870906829834 | 64 / 60000\n",
      "Loss: 1.1601037979125977 | 6464 / 60000\n",
      "Loss: 0.9793996214866638 | 12864 / 60000\n",
      "Loss: 1.1073204278945923 | 19264 / 60000\n",
      "Loss: 0.9879035353660583 | 25664 / 60000\n",
      "Loss: 1.022996187210083 | 32064 / 60000\n",
      "Loss: 1.052685022354126 | 38464 / 60000\n",
      "Loss: 0.9988136887550354 | 44864 / 60000\n",
      "Loss: 1.0291074514389038 | 51264 / 60000\n",
      "Loss: 0.9666974544525146 | 57664 / 60000\n",
      "Test || Loss: 0.982236 || Accuracy: 65.36%\n",
      "Epoch 7 \n",
      " ===============================================\n",
      "Loss: 1.0370848178863525 | 64 / 60000\n",
      "Loss: 1.0647729635238647 | 6464 / 60000\n",
      "Loss: 0.8650707602500916 | 12864 / 60000\n",
      "Loss: 1.0168249607086182 | 19264 / 60000\n",
      "Loss: 0.9016157388687134 | 25664 / 60000\n",
      "Loss: 0.9295880794525146 | 32064 / 60000\n",
      "Loss: 0.9766020774841309 | 38464 / 60000\n",
      "Loss: 0.9240753054618835 | 44864 / 60000\n",
      "Loss: 0.9504786133766174 | 51264 / 60000\n",
      "Loss: 0.9015414118766785 | 57664 / 60000\n",
      "Test || Loss: 0.910441 || Accuracy: 66.51%\n",
      "Epoch 8 \n",
      " ===============================================\n",
      "Loss: 0.9491667151451111 | 64 / 60000\n",
      "Loss: 0.998414158821106 | 6464 / 60000\n",
      "Loss: 0.7830401062965393 | 12864 / 60000\n",
      "Loss: 0.9526671767234802 | 19264 / 60000\n",
      "Loss: 0.8441371917724609 | 25664 / 60000\n",
      "Loss: 0.8612249493598938 | 32064 / 60000\n",
      "Loss: 0.9235873818397522 | 38464 / 60000\n",
      "Loss: 0.8739694952964783 | 44864 / 60000\n",
      "Loss: 0.8934366106987 | 51264 / 60000\n",
      "Loss: 0.8547353744506836 | 57664 / 60000\n",
      "Test || Loss: 0.858640 || Accuracy: 67.81%\n"
     ]
    }
   ],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(nn_model.parameters(), lr = 1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_func, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        preds = model(X)\n",
    "        preds_proba = torch.nn.Softmax(dim = 1)(preds)\n",
    "        loss = loss_func(preds, y)\n",
    "\n",
    "        #backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"Loss: {loss} | {current} / {size}\")\n",
    "\n",
    "def test(dataloader, model, loss_func):\n",
    "    size = len(dataloader.dataset)\n",
    "    batches_amount = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            preds = model(X)\n",
    "            test_loss += loss_func(preds, y).item()\n",
    "            correct += (preds.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= batches_amount\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test || Loss: {test_loss:>5f} || Accuracy: {correct*100:>.2f}%\")\n",
    "\n",
    "\n",
    "epochs = 8\n",
    "for e in range(epochs):\n",
    "    print(f\"Epoch {e+1} \\n ===============================================\")\n",
    "    train(train_dataloader, nn_model, loss_func, optimizer)\n",
    "    test(test_dataloader, nn_model, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7515c-b3b6-4643-b48b-f390e8b375dc",
   "metadata": {},
   "source": [
    "## state_dict ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f944cd72-d9d3-43cc-bbaf-3d9c77fc0a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_relu_stack.0.weight \t torch.Size([512, 784])\n",
      "linear_relu_stack.0.bias \t torch.Size([512])\n",
      "linear_relu_stack.2.weight \t torch.Size([512, 512])\n",
      "linear_relu_stack.2.bias \t torch.Size([512])\n",
      "linear_relu_stack.4.weight \t torch.Size([10, 512])\n",
      "linear_relu_stack.4.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in nn_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", nn_model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d48baa-6d3a-4f4a-ae19-2deef0f89547",
   "metadata": {},
   "source": [
    "## Свёрточная нейронная сеть (+ использование нормирования)\n",
    "\n",
    "Для обучения использовалась Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "071789cf-3a44-448f-9931-bdb9e23443a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [ToTensor(),\n",
    "    torchvision.transforms.Normalize((.5), (.5))]\n",
    ")\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = \"data_cnn\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root = \"data_cnn\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f07934d3-c5f7-465b-b6d2-3f669f3e5ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: torch.Size([64, 1, 28, 28]) | y_test.shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size = batch_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size = batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"X_test.shape: {X.shape} | y_test.shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a08442-846f-4346-82c1-60b8c9d726cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNet, self).__init__()\n",
    "        \n",
    "        self.first_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        )\n",
    "\n",
    "        self.second_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 0),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size = 2),\n",
    "        )\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(in_features = 64*6*6, out_features = 600)\n",
    "        self.drop = torch.nn.Dropout(.25) #drop neurons with probability .25\n",
    "        self.fc2 = torch.nn.Linear(in_features = 600, out_features = 120)\n",
    "        self.fc3 = torch.nn.Linear(in_features = 120, out_features = 10)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        out_sequential1 = self.first_layer(X)\n",
    "        out_sequential2 = self.second_layer(out_sequential1)\n",
    "        out_sequential2 = out_sequential2.view(out_sequential2.size(0), -1)\n",
    "        out_fc1 = self.fc1(out_sequential2)\n",
    "        out_drop = self.drop(out_fc1)\n",
    "        out_fc2 = self.fc2(out_drop)\n",
    "        out_fc3 = self.fc3(out_fc2)\n",
    "\n",
    "        return out_fc3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3bebd5-0e04-402e-9f4b-0f046998bd40",
   "metadata": {},
   "source": [
    "## Переопределим некоторые уже известные переменные для удобства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ed954-6ed7-42ff-b62a-4bbeb4bd8934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalNet(\n",
      "  (first_layer): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (second_layer): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=2304, out_features=600, bias=True)\n",
      "  (drop): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=600, out_features=120, bias=True)\n",
      "  (fc3): Linear(in_features=120, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_model = ConvolutionalNet().to(device)\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0a7442-f9a6-4cf5-9808-2df96f70a609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      " ===================================\n",
      "Loss: 2.339294910430908 | 64 / 60000\n",
      "Loss: 0.452540785074234 | 6464 / 60000\n",
      "Loss: 0.30578184127807617 | 12864 / 60000\n",
      "Loss: 0.5669863224029541 | 19264 / 60000\n",
      "Loss: 0.44785287976264954 | 25664 / 60000\n",
      "Loss: 0.5179901719093323 | 32064 / 60000\n",
      "Loss: 0.2588045299053192 | 38464 / 60000\n",
      "Loss: 0.543664276599884 | 44864 / 60000\n",
      "Loss: 0.39096739888191223 | 51264 / 60000\n",
      "Loss: 0.22987543046474457 | 57664 / 60000\n",
      "Test || Loss: 0.376832 || Accuracy: 86.73%\n",
      "Epoch 2 \n",
      " ===================================\n",
      "Loss: 0.23029367625713348 | 64 / 60000\n",
      "Loss: 0.299391508102417 | 6464 / 60000\n",
      "Loss: 0.1706864833831787 | 12864 / 60000\n",
      "Loss: 0.43205392360687256 | 19264 / 60000\n",
      "Loss: 0.5364901423454285 | 25664 / 60000\n",
      "Loss: 0.4061213433742523 | 32064 / 60000\n",
      "Loss: 0.20886099338531494 | 38464 / 60000\n",
      "Loss: 0.4511266052722931 | 44864 / 60000\n",
      "Loss: 0.29469287395477295 | 51264 / 60000\n",
      "Loss: 0.23984776437282562 | 57664 / 60000\n",
      "Test || Loss: 0.289221 || Accuracy: 89.89%\n",
      "Epoch 3 \n",
      " ===================================\n",
      "Loss: 0.15684136748313904 | 64 / 60000\n",
      "Loss: 0.3420168459415436 | 6464 / 60000\n",
      "Loss: 0.15903671085834503 | 12864 / 60000\n",
      "Loss: 0.34284713864326477 | 19264 / 60000\n",
      "Loss: 0.47162288427352905 | 25664 / 60000\n",
      "Loss: 0.35979610681533813 | 32064 / 60000\n",
      "Loss: 0.22025011479854584 | 38464 / 60000\n",
      "Loss: 0.3838232457637787 | 44864 / 60000\n",
      "Loss: 0.2145259976387024 | 51264 / 60000\n",
      "Loss: 0.24068506062030792 | 57664 / 60000\n",
      "Test || Loss: 0.311760 || Accuracy: 89.89%\n",
      "Epoch 4 \n",
      " ===================================\n",
      "Loss: 0.15002867579460144 | 64 / 60000\n",
      "Loss: 0.28598248958587646 | 6464 / 60000\n",
      "Loss: 0.1580529808998108 | 12864 / 60000\n",
      "Loss: 0.23860898613929749 | 19264 / 60000\n",
      "Loss: 0.41292697191238403 | 25664 / 60000\n",
      "Loss: 0.3623906970024109 | 32064 / 60000\n",
      "Loss: 0.22700558602809906 | 38464 / 60000\n",
      "Loss: 0.3702029883861542 | 44864 / 60000\n",
      "Loss: 0.19236399233341217 | 51264 / 60000\n",
      "Loss: 0.22264313697814941 | 57664 / 60000\n",
      "Test || Loss: 0.316694 || Accuracy: 89.74%\n",
      "Epoch 5 \n",
      " ===================================\n",
      "Loss: 0.16185985505580902 | 64 / 60000\n",
      "Loss: 0.2551850974559784 | 6464 / 60000\n",
      "Loss: 0.16870343685150146 | 12864 / 60000\n",
      "Loss: 0.20526279509067535 | 19264 / 60000\n",
      "Loss: 0.24262714385986328 | 25664 / 60000\n",
      "Loss: 0.3268832266330719 | 32064 / 60000\n",
      "Loss: 0.20116600394248962 | 38464 / 60000\n",
      "Loss: 0.3569403886795044 | 44864 / 60000\n",
      "Loss: 0.19186516106128693 | 51264 / 60000\n",
      "Loss: 0.22024893760681152 | 57664 / 60000\n",
      "Test || Loss: 0.318892 || Accuracy: 89.55%\n"
     ]
    }
   ],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr = 1e-3)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    print(f\"Epoch {e+1} \\n ===================================\")\n",
    "    train(train_dataloader, cnn_model, loss_func, optimizer)\n",
    "    test(test_dataloader, cnn_model, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511932e-ff5b-469a-ad66-b3f068d40692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_layer.0.weight \t torch.Size([32, 1, 3, 3])\n",
      "first_layer.0.bias \t torch.Size([32])\n",
      "first_layer.1.weight \t torch.Size([32])\n",
      "first_layer.1.bias \t torch.Size([32])\n",
      "first_layer.1.running_mean \t torch.Size([32])\n",
      "first_layer.1.running_var \t torch.Size([32])\n",
      "first_layer.1.num_batches_tracked \t torch.Size([])\n",
      "second_layer.0.weight \t torch.Size([64, 32, 3, 3])\n",
      "second_layer.0.bias \t torch.Size([64])\n",
      "second_layer.1.weight \t torch.Size([64])\n",
      "second_layer.1.bias \t torch.Size([64])\n",
      "second_layer.1.running_mean \t torch.Size([64])\n",
      "second_layer.1.running_var \t torch.Size([64])\n",
      "second_layer.1.num_batches_tracked \t torch.Size([])\n",
      "fc1.weight \t torch.Size([600, 2304])\n",
      "fc1.bias \t torch.Size([600])\n",
      "fc2.weight \t torch.Size([120, 600])\n",
      "fc2.bias \t torch.Size([120])\n",
      "fc3.weight \t torch.Size([10, 120])\n",
      "fc3.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in cnn_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", cnn_model.state_dict()[param_tensor].size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
